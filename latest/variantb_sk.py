# -*- coding: utf-8 -*-
"""VariantB_SK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C-WFSJAr6bNTg5keTFpgFNUZkIE_kEZ6
"""

# ============================
# CELL 1: Imports & Setup
# ============================
import os
import random
import glob

import numpy as np
import cv2
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# For progress bars (optional)
!pip install -q tqdm
from tqdm import tqdm

# ---- Reproducibility ----
def set_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seeds(42)

# ---- Device ----
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ============================
# CELL 2: Mount Drive & Paths
# ============================
from google.colab import drive
drive.mount("/content/drive")

# TODO: update this path if your folder is different
ROOT_DIR = "/content/drive/MyDrive/Five-Billion-Pixels"

IMG_DIR = os.path.join(ROOT_DIR, "Image__8bit_NirRGB")
MSK_DIR = os.path.join(ROOT_DIR, "Annotation__index")

print("IMG_DIR:", IMG_DIR)
print("MSK_DIR:", MSK_DIR)


# ----------------------------------------------
# LOAD IMAGES (TIFF)
# ----------------------------------------------
img_files = sorted(glob.glob(os.path.join(IMG_DIR, "*.tif")))
if len(img_files) == 0:
    img_files = sorted(glob.glob(os.path.join(IMG_DIR, "*.tiff")))

print("Number of images:", len(img_files))


# ----------------------------------------------
# LOAD MASKS (PNG)
# ----------------------------------------------
msk_files = sorted(glob.glob(os.path.join(MSK_DIR, "*.png")))

# Fallback if jpg or no extension (optional)
if len(msk_files) == 0:
    msk_files = sorted(glob.glob(os.path.join(MSK_DIR, "*.jpg")))
if len(msk_files) == 0:
    msk_files = sorted(glob.glob(os.path.join(MSK_DIR, "*")))

print("Number of masks:", len(msk_files))

# ----------------------------------------------
# Example prints
# ----------------------------------------------
print("Example image:", img_files[0] if img_files else "None")
print("Example mask :", msk_files[0] if msk_files else "None")


# ----------------------------------------------
# Final warning if mismatch
# ----------------------------------------------
if len(img_files) != len(msk_files):
    print("⚠ WARNING: Number of images and masks do NOT match!")
else:
    print("✓ Image-mask counts match.")

# ============================
# CELL 3: Load & Tile
# ============================

TILE_SIZE = 256
MAX_TILES_PER_IMAGE = 300      # cap per image
MAX_TOTAL_TILES     = 4000     # overall cap


def load_pair(img_path, msk_path):
    """
    Load a 4-channel image (NIR + RGB) and a mask.
    Returns:
        img: (H, W, 4) float32, normalized to [0,1], channels = [R,G,B,NIR]
        msk: (H, W) int64
    """
    # Read 4-channel tiff: expects order [NIR, R, G, B]
    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)  # (H,W,4)
    if img is None:
        raise ValueError(f"Failed to read image: {img_path}")
    if img.shape[-1] != 4:
        raise ValueError(f"Expected 4 channels, got {img.shape} in {img_path}")

    nir = img[:, :, 0]
    r   = img[:, :, 1]
    g   = img[:, :, 2]
    b   = img[:, :, 3]

    # Reorder to [R,G,B,NIR] and normalize
    img = np.stack([r, g, b, nir], axis=-1).astype("float32") / 255.0

    # Load mask
    msk = cv2.imread(msk_path, cv2.IMREAD_UNCHANGED)  # (H,W) or (H,W,3)
    if msk is None:
        raise ValueError(f"Failed to read mask: {msk_path}")

    if msk.ndim == 3:
        msk = msk[:, :, 0]  # if colored indexed TIFF, take one channel

    msk = msk.astype("int64")
    return img, msk


def make_tiles(img, msk, tile_size=TILE_SIZE):
    """
    Given full image & mask, cut into non-overlapping tiles of size tile_size x tile_size.
    img: (H,W,4)
    msk: (H,W)
    Returns list of tiles.
    """
    H, W, _ = img.shape
    tiles_img = []
    tiles_msk = []

    for y in range(0, H, tile_size):
        for x in range(0, W, tile_size):
            if y + tile_size <= H and x + tile_size <= W:
                tile_img = img[y:y+tile_size, x:x+tile_size, :]  # (256,256,4)
                tile_msk = msk[y:y+tile_size, x:x+tile_size]     # (256,256)
                tiles_img.append(tile_img)
                tiles_msk.append(tile_msk)

    return tiles_img, tiles_msk


# ---- Generate tiles for the first N images (if you want to limit) ----
NUM_IMAGES_TO_USE = 10   # you can increase later

all_imgs_tiles = []
all_msks_tiles = []

for i in range(min(NUM_IMAGES_TO_USE, len(img_files))):
    print(f"Processing image {i}:")
    img, msk = load_pair(img_files[i], msk_files[i])
    ti, tm = make_tiles(img, msk)

    print(f"  -> originally {len(ti)} tiles")

    # Convert to arrays temporarily to sample
    ti = np.array(ti, dtype="float32")
    tm = np.array(tm, dtype="int64")

    # Limit tiles per image
    if len(ti) > MAX_TILES_PER_IMAGE:
        idx = np.random.choice(len(ti), MAX_TILES_PER_IMAGE, replace=False)
        ti = ti[idx]
        tm = tm[idx]
        print(f"  -> downsampled to {len(ti)} tiles")

    all_imgs_tiles.extend(list(ti))
    all_msks_tiles.extend(list(tm))

    # Overall cap
    if len(all_imgs_tiles) >= MAX_TOTAL_TILES:
        print("Reached MAX_TOTAL_TILES cap.")
        break

all_imgs_tiles = np.array(all_imgs_tiles, dtype="float32")   # (N,256,256,4)
all_msks_tiles = np.array(all_msks_tiles, dtype="int64")     # (N,256,256)

print("Total tiles (images):", all_imgs_tiles.shape)
print("Total tiles (masks) :", all_msks_tiles.shape)

# ============================
# CELL 4: Dataset & DataLoader (updated, no stratify)
# ============================

from sklearn.model_selection import train_test_split

# ---- Train/Val split ----
X_train, X_val, y_train, y_val = train_test_split(
    all_imgs_tiles,
    all_msks_tiles,
    test_size=0.2,
    random_state=42,
    shuffle=True
)

print("Train tiles:", X_train.shape)
print("Val tiles  :", X_val.shape)


class SegmentationDataset(Dataset):
    def __init__(self, images, masks, augment=False):
        """
        images: (N,H,W,4), float32
        masks : (N,H,W), int64
        """
        self.images = images
        self.masks = masks
        self.augment = augment

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]   # (H,W,4)
        mask  = self.masks[idx]    # (H,W)

        # to torch: (4,H,W)
        image = torch.from_numpy(image).permute(2, 0, 1)   # (4,256,256)
        mask  = torch.from_numpy(mask).long()              # (256,256)

        # ---- Simple augmentations ----
        if self.augment:
            # horizontal flip
            if random.random() < 0.5:
                image = torch.flip(image, dims=[2])  # flip width
                mask  = torch.flip(mask,  dims=[1])

            # vertical flip
            if random.random() < 0.5:
                image = torch.flip(image, dims=[1])
                mask  = torch.flip(mask,  dims=[0])

            # 90° rotation
            if random.random() < 0.5:
                image = image.permute(0, 2, 1)
                mask  = mask.permute(1, 0)

            # brightness/contrast jitter (very simple)
            if random.random() < 0.3:
                factor = 0.9 + 0.2 * random.random()  # [0.9, 1.1]
                image = image * factor
                image = torch.clamp(image, 0.0, 1.0)

        return image, mask


BATCH_SIZE = 4

train_dataset = SegmentationDataset(X_train, y_train, augment=True)
val_dataset   = SegmentationDataset(X_val,   y_val,   augment=False)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

print("Train batches:", len(train_loader))
print("Val batches  :", len(val_loader))

# ============================
# CELL 5: SKConv & SKBottleneck
# ============================

class SKConv(nn.Module):
    """
    Simplified Selective Kernel convolution.
    M = number of branches (e.g. 2: 3x3 and 5x5).
    """
    def __init__(self, in_channels, out_channels, stride=1, M=2, G=32, r=16, L=32):
        super(SKConv, self).__init__()
        d = max(in_channels // r, L)
        self.M = M
        self.out_channels = out_channels

        # Multi-branch convs (3x3, 5x5)
        self.convs = nn.ModuleList()
        for i in range(M):
            k = 3 + 2 * i     # 3, 5, ...
            padding = k // 2
            self.convs.append(
                nn.Conv2d(
                    in_channels,
                    out_channels,
                    kernel_size=k,
                    stride=stride,
                    padding=padding,
                    groups=G,
                    bias=False
                )
            )

        self.bn = nn.BatchNorm2d(out_channels)

        # Squeeze
        self.fc = nn.Linear(out_channels, d)
        # Excitation branches
        self.fcs = nn.ModuleList()
        for _ in range(M):
            self.fcs.append(nn.Linear(d, out_channels))

        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        batch_size = x.size(0)

        # Step 1: multi-branch convs
        feats = []
        for conv in self.convs:
            feats.append(conv(x))

        # Sum for shortcut aggregation
        U = sum(feats)              # (B, C, H, W)
        U = self.bn(U)

        # Step 2: global pooling
        s = F.adaptive_avg_pool2d(U, 1).view(batch_size, -1)   # (B, C)

        # Step 3: squeeze
        z = self.fc(s)             # (B, d)
        z = F.relu(z, inplace=True)

        # Step 4: excitation for each branch
        attention_vectors = []
        for fc in self.fcs:
            attention_vectors.append(fc(z).unsqueeze(1))  # (B,1,C)

        attention = torch.cat(attention_vectors, dim=1)   # (B, M, C)
        attention = self.softmax(attention)               # softmax across M

        # Step 5: fuse
        attention = attention.unsqueeze(-1).unsqueeze(-1) # (B,M,C,1,1)

        feats_stack = torch.stack(feats, dim=1)           # (B,M,C,H,W)
        V = (attention * feats_stack).sum(dim=1)          # (B,C,H,W)

        return V


class SKBottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=32, base_width=4):
        """
        inplanes: input channels
        planes:   base planes (before expansion)
        stride:   stride for the SKConv (for downsampling)
        """
        super(SKBottleneck, self).__init__()
        width = int(planes * (base_width / 64.0)) * groups

        # 1x1 reduce
        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width)

        # SKConv instead of normal 3x3
        self.conv2 = SKConv(width, width, stride=stride, M=2, G=groups)
        self.bn2 = nn.BatchNorm2d(width)

        # 1x1 expand
        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)

        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

# ============================
# CELL 6: SK-ResNeXt backbone (4-ch)
# ============================

class SKResNeXt(nn.Module):
    """
    SK-ResNeXt-50 style backbone (3,4,6,3 blocks) with 4-channel input support.
    """
    def __init__(self, block, layers, num_classes=1000, in_channels=4, groups=32, width_per_group=4):
        super(SKResNeXt, self).__init__()
        self.inplanes = 64
        self.groups = groups
        self.base_width = width_per_group

        # NOTE: conv1 has in_channels=4 now
        self.conv1 = nn.Conv2d(
            in_channels, 64,
            kernel_size=7, stride=2, padding=3, bias=False
        )
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Encoder stages
        self.layer1 = self._make_layer(block,  64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        # (Classifier head not used for segmentation, but kept for completeness)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        # Init
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Linear)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None

        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.inplanes,
                    planes * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False
                ),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes, planes,
                stride=stride,
                downsample=downsample,
                groups=self.groups,
                base_width=self.base_width
            )
        )
        self.inplanes = planes * block.expansion

        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes, planes,
                    stride=1,
                    downsample=None,
                    groups=self.groups,
                    base_width=self.base_width
                )
            )

        return nn.Sequential(*layers)

    def forward(self, x):
        # We'll actually use the individual layers in the UNet, not fc
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)      # x0
        x0 = x

        x = self.maxpool(x)

        x1 = self.layer1(x)   # 256 ch
        x2 = self.layer2(x1)  # 512 ch
        x3 = self.layer3(x2)  # 1024 ch
        x4 = self.layer4(x3)  # 2048 ch

        return x0, x1, x2, x3, x4


def sk_resnext50_4ch():
    """
    Helper function to construct SK-ResNeXt-50-like backbone with 4-channel input.
    """
    model = SKResNeXt(
        block=SKBottleneck,
        layers=[3, 4, 6, 3],  # ResNet-50 / ResNeXt-50 config
        in_channels=4,
        groups=32,
        width_per_group=4
    )
    return model

# ============================
# CELL 7: Variant B (U-Net++ Dense Decoder + Deep Supervision)
# ============================

class ConvBlock(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()
        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False)
        self.bn1   = nn.BatchNorm2d(out_c)
        self.relu  = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False)
        self.bn2   = nn.BatchNorm2d(out_c)

    def forward(self, x):
        x = self.conv1(x); x = self.bn1(x); x = self.relu(x)
        x = self.conv2(x); x = self.bn2(x); x = self.relu(x)
        return x


class VariantB_SKResNeXt_UNetPP_DS(nn.Module):
    """
    Encoder: SK-ResNeXt-50 (4-ch) [same as Variant A]
    Decoder: U-Net++ (dense skip paths / nested decoder)
    Deep Supervision: outputs from x0_1, x0_2, x0_3, x0_4 (all upsampled to full res)
    """
    def __init__(self, n_classes=25):
        super().__init__()

        # ---- Encoder (same backbone as Variant A) ----
        self.backbone = sk_resnext50_4ch()

        self.enc0_conv = self.backbone.conv1
        self.enc0_bn   = self.backbone.bn1
        self.enc0_relu = self.backbone.relu
        self.pool0     = self.backbone.maxpool

        self.enc1 = self.backbone.layer1   # 256 ch
        self.enc2 = self.backbone.layer2   # 512 ch
        self.enc3 = self.backbone.layer3   # 1024 ch
        self.enc4 = self.backbone.layer4   # 2048 ch

        # ---- Channel adapters (IMPORTANT for U-Net++ to stay GPU-friendly) ----
        # We do NOT change encoder weights; we only project features before dense decoder.
        self.ad0 = nn.Conv2d(64,   32, kernel_size=1, bias=False)   # H/2
        self.ad1 = nn.Conv2d(256,  64, kernel_size=1, bias=False)   # H/4
        self.ad2 = nn.Conv2d(512, 128, kernel_size=1, bias=False)   # H/8
        self.ad3 = nn.Conv2d(1024,256, kernel_size=1, bias=False)   # H/16
        self.ad4 = nn.Conv2d(2048,512, kernel_size=1, bias=False)   # H/32

        # Upsample ops
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.up_to_full = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # H/2 -> H

        # ---- U-Net++ dense decoder nodes ----
        # Level naming:
        # x3_* : H/16 (256 ch)
        # x2_* : H/8  (128 ch)
        # x1_* : H/4  (64 ch)
        # x0_* : H/2  (32 ch)

        # First nested column (j=1)
        self.x3_1 = ConvBlock(256 + 512, 256)   # [x3_0, up(x4_0)]
        self.x2_1 = ConvBlock(128 + 256, 128)   # [x2_0, up(x3_0)]
        self.x1_1 = ConvBlock(64  + 128, 64)    # [x1_0, up(x2_0)]
        self.x0_1 = ConvBlock(32  + 64,  32)    # [x0_0, up(x1_0)]

        # Second nested column (j=2)
        self.x2_2 = ConvBlock(128 + 128 + 256, 128)  # [x2_0, x2_1, up(x3_1)]
        self.x1_2 = ConvBlock(64  + 64  + 128, 64)   # [x1_0, x1_1, up(x2_1)]
        self.x0_2 = ConvBlock(32  + 32  + 64,  32)   # [x0_0, x0_1, up(x1_1)]

        # Third nested column (j=3)
        self.x1_3 = ConvBlock(64 + 64 + 64 + 128, 64)   # [x1_0, x1_1, x1_2, up(x2_2)]
        self.x0_3 = ConvBlock(32 + 32 + 32 + 64,  32)   # [x0_0, x0_1, x0_2, up(x1_2)]

        # Fourth nested column (j=4)
        self.x0_4 = ConvBlock(32 + 32 + 32 + 32 + 64, 32)  # [x0_0, x0_1, x0_2, x0_3, up(x1_3)]

        # Final refinement at full resolution (same spirit as Variant A)
        self.final_refine = ConvBlock(32, 32)

        # ---- Deep Supervision heads (all predict at full resolution HxW) ----
        self.out1 = nn.Conv2d(32, n_classes, kernel_size=1)  # from x0_1
        self.out2 = nn.Conv2d(32, n_classes, kernel_size=1)  # from x0_2
        self.out3 = nn.Conv2d(32, n_classes, kernel_size=1)  # from x0_3
        self.out4 = nn.Conv2d(32, n_classes, kernel_size=1)  # from x0_4 (final)

    def forward(self, x):
        # ---- Encoder (same feature extraction as Variant A) ----
        x0 = self.enc0_conv(x)
        x0 = self.enc0_bn(x0)
        x0 = self.enc0_relu(x0)      # (B,64,H/2,W/2)

        p0 = self.pool0(x0)          # (B,64,H/4,W/4)
        x1 = self.enc1(p0)           # (B,256,H/4,W/4)
        x2 = self.enc2(x1)           # (B,512,H/8,W/8)
        x3 = self.enc3(x2)           # (B,1024,H/16,W/16)
        x4 = self.enc4(x3)           # (B,2048,H/32,W/32)

        # ---- Adapt channels (keeps decoder manageable) ----
        x0_0 = self.ad0(x0)          # (B,32,H/2,W/2)
        x1_0 = self.ad1(x1)          # (B,64,H/4,W/4)
        x2_0 = self.ad2(x2)          # (B,128,H/8,W/8)
        x3_0 = self.ad3(x3)          # (B,256,H/16,W/16)
        x4_0 = self.ad4(x4)          # (B,512,H/32,W/32)

        # ---- U-Net++ dense decoder ----
        x3_1 = self.x3_1(torch.cat([x3_0, self.up(x4_0)], dim=1))

        x2_1 = self.x2_1(torch.cat([x2_0, self.up(x3_0)], dim=1))
        x2_2 = self.x2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], dim=1))

        x1_1 = self.x1_1(torch.cat([x1_0, self.up(x2_0)], dim=1))
        x1_2 = self.x1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], dim=1))
        x1_3 = self.x1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], dim=1))

        x0_1 = self.x0_1(torch.cat([x0_0, self.up(x1_0)], dim=1))
        x0_2 = self.x0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], dim=1))
        x0_3 = self.x0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], dim=1))
        x0_4 = self.x0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], dim=1))

        # full-res refinement (H/2 -> H)
        full = self.up_to_full(x0_4)      # (B,32,H,W)
        full = self.final_refine(full)    # (B,32,H,W)

        # ---- Deep supervision outputs ----
        # x0_1/x0_2/x0_3 are H/2, so upsample them to H for supervision
        out1 = self.out1(self.up_to_full(x0_1))
        out2 = self.out2(self.up_to_full(x0_2))
        out3 = self.out3(self.up_to_full(x0_3))
        out4 = self.out4(full)            # final output

        return (out1, out2, out3, out4)


# Instantiate model (Variant B)
N_CLASSES = 25  # change if needed
model = VariantB_SKResNeXt_UNetPP_DS(n_classes=N_CLASSES).to(device)

print("Variant B Model initialized. Total parameters:",
      sum(p.numel() for p in model.parameters()) / 1e6, "M")

# ============================
# CELL 8: Deep Supervision Loss + Train/Val (mIoU + Overall Acc + Time)
# ============================

import time
import torch
import torch.nn as nn
import torch.nn.functional as F

# ---------- Overall Accuracy ----------
@torch.no_grad()
def pixel_accuracy(logits, target):
    """
    logits: (B,C,H,W)
    target: (B,H,W)  long
    """
    pred = torch.argmax(logits, dim=1)
    correct = (pred == target).sum().item()
    total = target.numel()
    return correct / max(total, 1)


# ---------- mIoU (safe + simple) ----------
@torch.no_grad()
def miou_score(logits, target, num_classes):
    """
    Computes mean IoU across classes (ignoring classes not present in target+pred).
    logits: (B,C,H,W)
    target: (B,H,W)
    """
    pred = torch.argmax(logits, dim=1)  # (B,H,W)

    ious = []
    for cls in range(num_classes):
        pred_i = (pred == cls)
        tgt_i  = (target == cls)

        inter = (pred_i & tgt_i).sum().item()
        union = (pred_i | tgt_i).sum().item()

        if union == 0:
            continue  # skip absent class
        ious.append(inter / union)

    if len(ious) == 0:
        return 0.0
    return sum(ious) / len(ious)


# ---------- Dice Loss (multi-class) ----------
def dice_loss_multiclass(logits, target, num_classes, eps=1e-6):
    """
    logits: (B,C,H,W)
    target: (B,H,W) long
    """
    probs = torch.softmax(logits, dim=1)  # (B,C,H,W)
    target_1h = F.one_hot(target, num_classes=num_classes).permute(0,3,1,2).float()  # (B,C,H,W)

    dims = (0,2,3)
    inter = torch.sum(probs * target_1h, dims)
    denom = torch.sum(probs + target_1h, dims)

    dice_per_class = (2.0 * inter + eps) / (denom + eps)  # (C,)
    return 1.0 - dice_per_class.mean()


# ---------- Deep Supervision Loss Wrapper ----------
class DeepSupervisionLoss(nn.Module):
    """
    model returns: (out1, out2, out3, out4) each (B,C,H,W)
    Loss = Σ w_k * (CE + Dice)
    """
    def __init__(self, num_classes, ce_weight=None, ds_weights=(0.1, 0.2, 0.3, 0.4)):
        super().__init__()
        self.num_classes = num_classes
        self.ds_weights = ds_weights
        self.ce = nn.CrossEntropyLoss(weight=ce_weight)  # ce_weight can be None

    def forward(self, outputs, target):
        # outputs: tuple/list of 4 logits
        assert len(outputs) == 4, "Expected 4 outputs (out1,out2,out3,out4) from Variant B"
        total = 0.0
        for w, out in zip(self.ds_weights, outputs):
            ce_loss = self.ce(out, target)
            d_loss  = dice_loss_multiclass(out, target, self.num_classes)
            total  += w * (ce_loss + d_loss)
        return total


# ---- If you had class weights in Variant A, reuse them here.
# Example: ce_weight = torch.tensor(class_weights, dtype=torch.float32).to(device)
# If you DON'T have them, keep ce_weight=None.
try:
    ce_weight  # noqa
    print("Using existing ce_weight from previous cells.")
except NameError:
    ce_weight = None
    print("ce_weight not found -> using CE without class weights.")

criterion = DeepSupervisionLoss(num_classes=N_CLASSES, ce_weight=ce_weight, ds_weights=(0.1, 0.2, 0.3, 0.4))


# ---------- Train one epoch ----------
def train_one_epoch(model, loader, optimizer, criterion, device, num_classes, use_amp=True):
    model.train()
    t0 = time.time()

    total_loss = 0.0
    total_miou = 0.0
    total_acc  = 0.0
    n_batches  = 0

    for imgs, masks in loader:
        imgs  = imgs.to(device, non_blocking=True)
        masks = masks.to(device, non_blocking=True).long()

        optimizer.zero_grad(set_to_none=True)

        if use_amp and ('scaler' in globals()):
            with torch.cuda.amp.autocast():
                outs = model(imgs)              # (out1,out2,out3,out4)
                loss = criterion(outs, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outs = model(imgs)
            loss = criterion(outs, masks)
            loss.backward()
            optimizer.step()

        # Metrics on FINAL output only (out4)
        out_final = outs[-1]
        batch_miou = miou_score(out_final, masks, num_classes)
        batch_acc  = pixel_accuracy(out_final, masks)

        total_loss += loss.item()
        total_miou += batch_miou
        total_acc  += batch_acc
        n_batches  += 1

    epoch_time = time.time() - t0
    return (total_loss / n_batches,
            total_miou / n_batches,
            total_acc  / n_batches,
            epoch_time)


# ---------- Validate ----------
@torch.no_grad()
def validate(model, loader, criterion, device, num_classes, use_amp=True):
    model.eval()
    t0 = time.time()

    total_loss = 0.0
    total_miou = 0.0
    total_acc  = 0.0
    n_batches  = 0

    for imgs, masks in loader:
        imgs  = imgs.to(device, non_blocking=True)
        masks = masks.to(device, non_blocking=True).long()

        if use_amp and ('scaler' in globals()):
            with torch.cuda.amp.autocast():
                outs = model(imgs)
                loss = criterion(outs, masks)
        else:
            outs = model(imgs)
            loss = criterion(outs, masks)

        out_final  = outs[-1]
        batch_miou = miou_score(out_final, masks, num_classes)
        batch_acc  = pixel_accuracy(out_final, masks)

        total_loss += loss.item()
        total_miou += batch_miou
        total_acc  += batch_acc
        n_batches  += 1

    epoch_time = time.time() - t0
    return (total_loss / n_batches,
            total_miou / n_batches,
            total_acc  / n_batches,
            epoch_time)

# ============================
# CELL 8.5: Optimizer (and optional Scheduler + AMP Scaler)
# ============================

import torch

# --- Make sure model is on device already ---
model = model.to(device)

LR = 1e-4  # keep SAME as Variant A
WEIGHT_DECAY = 1e-4  # keep SAME as Variant A (or 0 if you used 0)

optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

# OPTIONAL: if you used a scheduler in Variant A, keep the same one here.
# Example: ReduceLROnPlateau (common for segmentation)
# from torch.optim.lr_scheduler import ReduceLROnPlateau
# scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)

scheduler = None  # keep None if you didn't use one

# OPTIONAL: AMP scaler (if you're on GPU and want mixed precision)
scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None

print("Optimizer ready:", optimizer)
print("Scheduler:", scheduler)
print("AMP scaler:", "ON" if scaler is not None else "OFF")

# ============================
# CELL 9: Training Loop (Variant B) + Best Checkpoint + Logs + Timing
# ============================

import os
import torch
import pandas as pd
from datetime import timedelta

# ---- If these exist in Variant A, keep them same; this is just safety fallback ----
try:
    EPOCHS
except NameError:
    EPOCHS = 40  # keep same as Variant A

# ---- Save path (edit only the filename if you want) ----
SAVE_DIR = "./checkpoints"
os.makedirs(SAVE_DIR, exist_ok=True)
best_ckpt_path = os.path.join(SAVE_DIR, "VariantB_best.pth")

# ---- History for final report plotting ----
history = {
    "epoch": [],
    "train_loss": [], "train_miou": [], "train_acc": [], "train_time_sec": [],
    "val_loss":   [], "val_miou":   [], "val_acc":   [], "val_time_sec":   []
}

best_val_miou = -1.0
total_start = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
total_end   = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None

# Optional: GPU-accurate total timing (still keep wall-clock per epoch from CELL 8)
if total_start is not None:
    torch.cuda.synchronize()
    total_start.record()

for epoch in range(1, EPOCHS + 1):
    # ---- Train ----
    tr_loss, tr_miou, tr_acc, tr_time = train_one_epoch(
        model=model,
        loader=train_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        num_classes=N_CLASSES,
        use_amp=True
    )

    # ---- Validate ----
    va_loss, va_miou, va_acc, va_time = validate(
        model=model,
        loader=val_loader,
        criterion=criterion,
        device=device,
        num_classes=N_CLASSES,
        use_amp=True
    )

    # ---- Scheduler (if you had one in Variant A, keep it same) ----
    # Common patterns:
    # 1) scheduler.step() per epoch
    # 2) ReduceLROnPlateau: scheduler.step(va_loss) or scheduler.step(va_miou)
    if "scheduler" in globals() and scheduler is not None:
        try:
            # if ReduceLROnPlateau-like
            scheduler.step(va_loss)
        except TypeError:
            # normal schedulers
            scheduler.step()

    # ---- Log history ----
    history["epoch"].append(epoch)
    history["train_loss"].append(tr_loss)
    history["train_miou"].append(tr_miou)
    history["train_acc"].append(tr_acc)
    history["train_time_sec"].append(tr_time)

    history["val_loss"].append(va_loss)
    history["val_miou"].append(va_miou)
    history["val_acc"].append(va_acc)
    history["val_time_sec"].append(va_time)

    # ---- Print logs (Variant A style + accuracy + time) ----
    print(f"\nEpoch {epoch}/{EPOCHS}")
    print(f"  Train Loss: {tr_loss:.4f} | Train mIoU: {tr_miou:.4f} | Train Acc: {tr_acc:.4f} | Time: {tr_time:.1f}s")
    print(f"  Val   Loss: {va_loss:.4f} | Val   mIoU: {va_miou:.4f} | Val   Acc: {va_acc:.4f} | Time: {va_time:.1f}s")

    # ---- Save best by Val mIoU (for fair comparison) ----
    if va_miou > best_val_miou:
        best_val_miou = va_miou
        torch.save({
            "epoch": epoch,
            "model_state": model.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "best_val_miou": best_val_miou,
            "history": history
        }, best_ckpt_path)
        print(f"  -> New best model saved with mIoU {best_val_miou:.4f}")

# ---- Total training time ----
if total_end is not None:
    total_end.record()
    torch.cuda.synchronize()
    total_ms = total_start.elapsed_time(total_end)
    total_sec = total_ms / 1000.0
else:
    # fallback: sum epoch times
    total_sec = sum(history["train_time_sec"]) + sum(history["val_time_sec"])

print("\n============================")
print(f"Training finished. Best Val mIoU: {best_val_miou:.4f}")
print(f"Total time (approx): {timedelta(seconds=int(total_sec))}")
print(f"Best checkpoint saved at: {best_ckpt_path}")
print("============================")

# ---- Save history CSV for your final report ----
hist_csv = os.path.join(SAVE_DIR, "VariantB_history.csv")
pd.DataFrame(history).to_csv(hist_csv, index=False)
print(f"History saved to: {hist_csv}")

# ============================
# CELL 10: Plotting Curves (Variant B ONLY)
# Loss / mIoU / Overall Accuracy / Time per Epoch
# ============================

import os
import pandas as pd
import matplotlib.pyplot as plt

# ---- Path to Variant B history ----
SAVE_DIR = "./checkpoints"
variantB_csv = os.path.join(SAVE_DIR, "VariantB_history.csv")

assert os.path.exists(variantB_csv), f"VariantB_history.csv not found at: {variantB_csv}"

histB = pd.read_csv(variantB_csv)
epochs = histB["epoch"].values

print("Loaded Variant B history:", variantB_csv)

def plot_one(x, y, title, ylabel, save_name=None):
    plt.figure()
    plt.plot(x, y)
    plt.xlabel("Epoch")
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(True)

    if save_name:
        out_path = os.path.join(SAVE_DIR, save_name)
        plt.savefig(out_path, dpi=200, bbox_inches="tight")
        print("Saved:", out_path)

    plt.show()

# ---- Loss ----
plot_one(epochs, histB["train_loss"].values, "Variant B: Train Loss", "Loss", "VariantB_TrainLoss.png")
plot_one(epochs, histB["val_loss"].values,   "Variant B: Val Loss",   "Loss", "VariantB_ValLoss.png")

# ---- mIoU ----
plot_one(epochs, histB["train_miou"].values, "Variant B: Train mIoU", "mIoU", "VariantB_TrainmIoU.png")
plot_one(epochs, histB["val_miou"].values,   "Variant B: Val mIoU",   "mIoU", "VariantB_ValmIoU.png")

# ---- Overall Accuracy ----
plot_one(epochs, histB["train_acc"].values,  "Variant B: Train Overall Accuracy", "Accuracy", "VariantB_TrainAcc.png")
plot_one(epochs, histB["val_acc"].values,    "Variant B: Val Overall Accuracy",   "Accuracy", "VariantB_ValAcc.png")

# ---- Time per Epoch (seconds) ----
plot_one(epochs, histB["train_time_sec"].values, "Variant B: Train Time per Epoch", "Seconds", "VariantB_TrainTime.png")
plot_one(epochs, histB["val_time_sec"].values,   "Variant B: Val Time per Epoch",   "Seconds", "VariantB_ValTime.png")

# ---- Best-epoch summary (for report) ----
best_idx = histB["val_miou"].values.argmax()
best_epoch = int(histB["epoch"].iloc[best_idx])
best_val_miou = float(histB["val_miou"].iloc[best_idx])
best_val_acc  = float(histB["val_acc"].iloc[best_idx])

total_train_time = float(histB["train_time_sec"].sum())
total_val_time   = float(histB["val_time_sec"].sum())
total_time       = total_train_time + total_val_time

print("\n===== Variant B Summary =====")
print(f"Best Epoch (by Val mIoU): {best_epoch}")
print(f"Best Val mIoU: {best_val_miou:.4f}")
print(f"Best Val Acc : {best_val_acc:.4f}")
print(f"Total Train Time (sec): {total_train_time:.1f}")
print(f"Total Val Time   (sec): {total_val_time:.1f}")
print(f"Total (Train+Val) (sec): {total_time:.1f}")
print("=============================\n")