{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-Temporal Adaptive Fusion Transformer (STAFT) - Variant B Training\n",
    "\n",
    "This notebook implements the training pipeline for **Variant B** (SK-ResNeXt-50 Encoder + U-Net++ Decoder with Deep Supervision) for Land Cover Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/STAFT_Project/checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install segmentation-models-pytorch albumentations timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model (Variant B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class VariantB(nn.Module):\n",
    "    \"\"\"\n",
    "    Variant B â€“ SK-ResNeXt Encoder + Dense (U-Net++) Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, classes=24, deep_supervision=True):\n",
    "        super(VariantB, self).__init__()\n",
    "        \n",
    "        # U-Net++ with SK-ResNeXt-50 encoder\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=\"skresnext50_32x4d\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            decoder_use_batchnorm=True,\n",
    "            deep_supervision=deep_supervision,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class DeepSupervisionLoss(nn.Module):\n",
    "    def __init__(self, weights=None, ignore_index=None):\n",
    "        super(DeepSupervisionLoss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=weights, ignore_index=ignore_index if ignore_index is not None else -100)\n",
    "        self.dice_loss = smp.losses.DiceLoss(mode='multiclass', ignore_index=ignore_index)\n",
    "\n",
    "    def forward(self, outputs, target):\n",
    "        loss = 0\n",
    "        # Deep supervision returns a list of outputs\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            for output in outputs:\n",
    "                ce = self.ce_loss(output, target)\n",
    "                dice = self.dice_loss(output, target)\n",
    "                loss += (ce + dice)\n",
    "            loss /= len(outputs)\n",
    "        else:\n",
    "            loss = self.ce_loss(outputs, target) + self.dice_loss(outputs, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # TODO: Update these paths to match your folder structure on Drive\n",
    "        # Example: /content/drive/MyDrive/STAFT_Project/data/train/images/*.tif\n",
    "        # self.image_paths = sorted(glob.glob(os.path.join(root_dir, split, 'images', '*.tif')))\n",
    "        # self.mask_paths = sorted(glob.glob(os.path.join(root_dir, split, 'masks', '*.tif')))\n",
    "        \n",
    "        # --- DUMMY DATA FOR TESTING (Remove when real data is ready) ---\n",
    "        self.image_paths = [\"dummy\"] * 100 \n",
    "        self.mask_paths = [\"dummy\"] * 100\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --- REAL LOADING LOGIC (Uncomment this) ---\n",
    "        # img_path = self.image_paths[idx]\n",
    "        # mask_path = self.mask_paths[idx]\n",
    "        # image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED) # 4 channels\n",
    "        # mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # --- DUMMY LOGIC (Remove this) ---\n",
    "        image = np.random.rand(256, 256, 4).astype(np.float32)\n",
    "        mask = np.random.randint(0, 24, (256, 256)).astype(np.uint8)\n",
    "        # ---------------------------------\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = '/content/drive/MyDrive/STAFT_Project/data'\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/STAFT_Project/checkpoints'\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LR = 5e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Prepare Data\n",
    "dataset = LandCoverDataset(root_dir=DATA_DIR, split='train')\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize Model\n",
    "model = VariantB(in_channels=4, classes=24, deep_supervision=True).to(DEVICE)\n",
    "criterion = DeepSupervisionLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training Function\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, masks in pbar:\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1)})\n",
    "        \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Accuracy on final output (assuming index 0 is final in deep supervision list)\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                final = outputs[0]\n",
    "            else:\n",
    "                final = outputs\n",
    "                \n",
    "            preds = torch.argmax(final, dim=1)\n",
    "            correct += (preds == masks).sum().item()\n",
    "            total += torch.numel(preds)\n",
    "            \n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "# Main Loop\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_model_variant_b.pth'))\n",
    "        print(\"Saved Best Model!\")\n",
    "        \n",
    "    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'last_model_variant_b.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
